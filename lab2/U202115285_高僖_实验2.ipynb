{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c012f734",
   "metadata": {},
   "source": [
    "# 上机实验二：基于卷积神经网络的MNIST手写体数字识别\n",
    "计卓2101 高僖 U202115285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7a161",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "对于MNIST手写数字数据集，我进行了如下预处理：\n",
    "\n",
    "1. **加载数据集：** 使用TensorFlow的Keras库加载MNIST数据集，其中包括训练集和测试集。\n",
    "\n",
    "2. **数据形状调整：** 调整图像的形状以适应模型的输入。原始图像是28x28的灰度图像，将其调整为大小为(28, 28, 1)的张量。\n",
    "\n",
    "3. **数据类型转换：** 将图像的像素值从整数转换为浮点数，并进行归一化处理，使得像素值在0到1之间。\n",
    "\n",
    "4. **标签独热编码：** 对训练集和测试集的标签进行独热编码处理，以便与模型输出的格式匹配。\n",
    "\n",
    "## 网络结构\n",
    "\n",
    "构建了一个CNN，其中包含了ResNet块。网络结构如下：\n",
    "\n",
    "1. **输入层：** 输入层接受大小为(28, 28, 1)的图像。\n",
    "\n",
    "2. **卷积层1：** 32个3x3的卷积核，ReLU激活函数，批量归一化。\n",
    "\n",
    "3. **ResNet块：** 包含两个卷积层，每个卷积层有32个 3x3 或者 5x5 的卷积核，使用了ReLU激活函数，批量归一化。\n",
    "\n",
    "4. **卷积层2：** 64个3x3的卷积核，ReLU激活函数，批量归一化。\n",
    "\n",
    "5. **池化层：** 最大池化层，池化窗口大小为2x2。\n",
    "\n",
    "6. **展平层：** 将卷积层输出的张量展平成一维向量。\n",
    "\n",
    "7. **全连接层1：** 128个神经元，ReLU激活函数，批量归一化。\n",
    "\n",
    "8. **输出层：** 10个神经元，使用Softmax激活函数进行多类别分类。\n",
    "\n",
    "### ResNet块\n",
    "\n",
    "#### 结构说明\n",
    "\n",
    "ResNet块包含两个卷积层，每个卷积层后跟一个批量归一化层和ReLU激活函数。为了引入残差连接，我在第一个卷积层的输出和第二个卷积层的输出之间添加了一个残差连接。\n",
    "\n",
    "1. **第一个卷积层：** 使用3x3 或者 5x5 的卷积核，ReLU激活函数，批量归一化。这一层旨在获取输入的特征。\n",
    "\n",
    "2. **第二个卷积层：** 同样使用3x3 或者 5x5 的卷积核，ReLU激活函数，批量归一化。这一层用来获取第一层的特征。\n",
    "\n",
    "3. **残差连接：** 如果两个卷积层的输出形状相同，将它们相加。如果不同，我们使用一个额外的1x1卷积层（残差卷积）来调整形状，确保可以相加。\n",
    "\n",
    "4. **ReLU激活：** 将残差连接的结果通过ReLU激活函数，引入非线性。\n",
    "\n",
    "#### 目的\n",
    "\n",
    "引入ResNet块的主要目的是解决梯度消失问题，特别是在训练深度网络时。由于残差连接，即使网络很深，模型也能够有效地学习恒等映射，从而避免了梯度逐渐减小到接近零的问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933a6d4",
   "metadata": {},
   "source": [
    "## 实验结果\n",
    "本次实验由于本机性能有限，只训练了一个epoch，但是也获得了不错的结果。对于不同网络结构的尝试，主要在于resnet模块的卷积核大小的更改。我分别使用了3x3和5x5的卷积核进行了训练。训练结果和分析如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e56b2",
   "metadata": {},
   "source": [
    "### 使用 3 * 3 卷积核\n",
    "\n",
    "#### 训练结果\n",
    "\n",
    "- **最终测试结果：**\n",
    "  - 测试损失：0.0645\n",
    "  - 测试准确率：98.06%\n",
    "\n",
    "- **最终训练结果：**\n",
    "  - 训练损失：0.0552\n",
    "  - 训练准确率：98.31%\n",
    "\n",
    "#### 类别准确率\n",
    "\n",
    "以下是测试集中每个类别的准确率：\n",
    "\n",
    "```\n",
    "Class 0: 99.08%\n",
    "Class 1: 99.65%\n",
    "Class 2: 98.45%\n",
    "Class 3: 99.90%\n",
    "Class 4: 99.19%\n",
    "Class 5: 96.52%\n",
    "Class 6: 98.33%\n",
    "Class 7: 99.61%\n",
    "Class 8: 93.43%\n",
    "Class 9: 95.94%\n",
    "```\n",
    "<img src=\"class_accuracy_bar_chart_3.png\" style=\"zoom:70%;\" />\n",
    "\n",
    "通过这些结果可以看出，模型在各个类别上都表现良好，尤其是在数字3的准确率最高，达到了99.90%。整体而言，该模型在MNIST手写数字分类任务上取得了很好的性能。\n",
    "\n",
    "#### 每一轮mini-batch后的损失\n",
    "\n",
    "<img src=\"loss_curves_3.png\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f61a08",
   "metadata": {},
   "source": [
    "### 使用 5*5 卷积核\n",
    "\n",
    "#### 训练结果\n",
    "\n",
    "- **最终测试结果：**\n",
    "  - 测试损失：0.0325\n",
    "  - 测试准确率：98.89%\n",
    "\n",
    "- **最终训练结果：**\n",
    "  - 训练损失：0.0274\n",
    "  - 训练准确率：99.13%\n",
    "\n",
    "#### 类别准确率\n",
    "\n",
    "以下是测试集中每个类别的准确率：\n",
    "\n",
    "```\n",
    "Class 0: 99.59%\n",
    "Class 1: 98.94%\n",
    "Class 2: 99.42%\n",
    "Class 3: 99.70%\n",
    "Class 4: 99.39%\n",
    "Class 5: 98.65%\n",
    "Class 6: 99.06%\n",
    "Class 7: 97.86%\n",
    "Class 8: 99.49%\n",
    "Class 9: 96.83%\n",
    "```\n",
    "<img src=\"class_accuracy_bar_chart_5.png\" style=\"zoom:70%;\" />\n",
    "\n",
    "通过这些结果可以看出，模型在各个类别上都表现良好\n",
    "\n",
    "#### 每一轮mini-batch后的损失\n",
    "\n",
    "<img src=\"loss_curves_5.png\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfc368",
   "metadata": {},
   "source": [
    "## 关于Loss曲线的分析\n",
    "\n",
    "不难发现，两种不同网络结构的loss曲线，特点相近：\n",
    "\n",
    "- **0~50 mini-batch：** 模型刚开始训练，loss 上升。据我分析，这应该是模型正在学习数据中的模式，参数还没有得到充分更新。\n",
    "\n",
    "- **50~100 mini-batch：** loss 开始下降，模型逐渐学到了数据的特征，性能提升。\n",
    "\n",
    "- **100~200 mini-batch：** loss 下降相对平稳，模型进一步收敛。这个阶段模型可能已经较好地学到了数据的特征。\n",
    "\n",
    "- **200~450 mini-batch：** loss 下降的过程中有一些波动，这可能是由于学习率较大或者数据中的噪声引起的。\n",
    "\n",
    "- **450~600 mini-batch：** loss 下降明显，模型性能进一步提升。此时可以考虑在模型训练的早期引入学习率衰减策略，以更好地控制训练的稳定性。\n",
    "\n",
    "- **600~900+ mini-batch：** loss 继续下降，最终趋于稳定。在这个阶段，可以观察到 loss 下降的幅度减缓，模型逐渐收敛到最优解。\n",
    "\n",
    "## 实验总结\n",
    "总体来说，一次epoch可以达到这个精度我还是很满意的。至于Resnet框架中的卷积核的大小是3还是5目前来看对精度影响不是很大，倒是对于训练时间影响不小：前者30分钟，后者45分钟，我认为以后设计网络还是得平衡准确度与网络的性能。当然了，我认为最影响性能的是每一轮mini-batch都要计算test loss。我自认为，这会导致在每个 mini-batch 后都进行一次完整的测试集前向传播和损失计算，从而显著增加计算量，导致训练速度变慢。\n",
    "\n",
    "至于Loss曲线，观察后也不难看出，在mini-batch 200-400附近，loss的下降有一些波动，如果有时间，我还是计划尝试减小学习率来平缓 loss 下降的过程。但是由于硬件限制，一次训练就花了不少时间，我决定在提交本次实验报告之前就不再重新训练了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3ed81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x000002179265F930>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faaf217",
   "metadata": {},
   "source": [
    "## 3 * 3 卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 定义ResNet模块\n",
    "class ResNetBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=3, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "\n",
    "        self.conv2 = layers.Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "\n",
    "        # 1x1 convolutional layer for the residual connection\n",
    "        if stride > 1:\n",
    "            self.residual_conv = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')\n",
    "        else:\n",
    "            self.residual_conv = None\n",
    "\n",
    "        self.relu2 = layers.Activation('relu')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "\n",
    "        # Apply residual connection if needed\n",
    "        if self.residual_conv is not None:\n",
    "            inputs = self.residual_conv(inputs)\n",
    "\n",
    "        x = layers.add([inputs, x])\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "# 构建卷积神经网络\n",
    "def build_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # 第一个卷积层\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # ResNet模块\n",
    "    model.add(ResNetBlock(32))\n",
    "\n",
    "    # 其他卷积层和池化层\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # 全连接层\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # 输出层\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# 加载MNIST数据集\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# 构建模型\n",
    "model = build_cnn_model((28, 28, 1))\n",
    "\n",
    "# 定义一个回调类来获取每个 mini-batch 后的训练损失和测试损失\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        test_loss = self.model.evaluate(test_images, test_labels, verbose=0)[0]\n",
    "        self.test_losses.append(test_loss)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        test_loss = self.model.evaluate(test_images, test_labels, verbose=0)[0]\n",
    "        self.test_losses.append(test_loss)\n",
    "        print(f'\\nEpoch {epoch + 1}, Test Loss: {test_loss}')\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 实例化 LossHistory 回调类\n",
    "loss_history = LossHistory()\n",
    "\n",
    "# 训练模型，并将 LossHistory 作为回调传递\n",
    "history = model.fit(train_images, train_labels, epochs=1, batch_size=64, validation_data=(test_images, test_labels), callbacks=[loss_history])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aea2973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 26, 26, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " res_net_block (ResNetBlock  (None, 26, 26, 32)        18752     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1179776   \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219530 (4.65 MB)\n",
      "Trainable params: 1218954 (4.65 MB)\n",
      "Non-trainable params: 576 (2.25 KB)\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0645 - accuracy: 0.9806\n",
      "\n",
      "Final Test Loss: 0.0645499974489212, Test Accuracy: 0.9805999994277954\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0552 - accuracy: 0.9831\n",
      "Final Training Loss: 0.05521545931696892, Training Accuracy: 0.9830999970436096\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Class Accuracy:\n",
      "    Class 0   Class 1   Class 2  Class 3   Class 4   Class 5   Class 6  \\\n",
      "0  0.990816  0.996476  0.984496  0.99901  0.991853  0.965247  0.983299   \n",
      "\n",
      "    Class 7   Class 8   Class 9  \n",
      "0  0.996109  0.934292  0.959366  \n"
     ]
    }
   ],
   "source": [
    "# 打印模型概要\n",
    "model.summary()\n",
    "\n",
    "# 输出最终测试损失和准确率\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'\\nFinal Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n",
    "\n",
    "# 输出最终训练损失和准确率\n",
    "train_loss, train_acc = model.evaluate(train_images, train_labels)\n",
    "print(f'Final Training Loss: {train_loss}, Training Accuracy: {train_acc}')\n",
    "\n",
    "# 计算测试集中每一类的准确率\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = tf.argmax(predictions, axis=1)\n",
    "true_labels = tf.argmax(test_labels, axis=1)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(true_labels, predicted_labels, num_classes=10)\n",
    "class_accuracy = tf.linalg.diag_part(confusion_matrix) / tf.reduce_sum(confusion_matrix, axis=1)\n",
    "\n",
    "class_accuracy_dict = {f'Class {i}': [acc.numpy()] for i, acc in enumerate(class_accuracy)}\n",
    "\n",
    "# 创建 Pandas DataFrame\n",
    "df = pd.DataFrame(class_accuracy_dict)\n",
    "\n",
    "# 显示表格\n",
    "print(\"Class Accuracy:\")\n",
    "print(df)\n",
    "\n",
    "# 将表格写入 CSV 文件\n",
    "df.to_csv('class_accuracy_3.csv', index=False)\n",
    "\n",
    "# 绘制条形图\n",
    "plt.figure(figsize=(10, 6))  # 设置图形大小\n",
    "\n",
    "bars = plt.bar(df.columns, df.iloc[0])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Class-wise Accuracy with 3x3 Convolutional Kernel')  # 添加标题\n",
    "plt.xticks(rotation=45)  # 使x轴标签斜着显示\n",
    "\n",
    "# 在条形图上显示具体的数值\n",
    "for bar, label in zip(bars, df.iloc[0]):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2 - 0.15, bar.get_height() + 0.01, f'{label:.2%}', ha='center', va='bottom')\n",
    "\n",
    "# 保存条形图\n",
    "plt.savefig('class_accuracy_bar_chart_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad956714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练过程中的损失曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 绘制训练损失曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history.train_losses, label='Training Loss')\n",
    "plt.xlabel('Mini-Batch / Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在每50个点（i < 600）或每200个点（i >= 600）上显示具体的数值\n",
    "for i, value in enumerate(loss_history.train_losses):\n",
    "    if i < 600 and i % 50 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "    elif i >= 600 and (i - 600) % 200 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 绘制测试损失曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history.test_losses, label='Test Loss', color='orange')\n",
    "plt.xlabel('Mini-Batch / Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在每50个点（i < 600）或每200个点（i >= 600）上显示具体的数值\n",
    "for i, value in enumerate(loss_history.test_losses):\n",
    "    if i < 600 and i % 50 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "    elif i >= 600 and (i - 600) % 200 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 在图形上方添加总标题\n",
    "plt.suptitle('Loss Curves with 3x3 Convolutional Kernel', fontsize=16, y=1)\n",
    "\n",
    "# 保存损失曲线图\n",
    "plt.savefig('loss_curves_3.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566972c",
   "metadata": {},
   "source": [
    "## 5 * 5 卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545923a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gavin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gavin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gavin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gavin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gavin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gavin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "  6/938 [..............................] - ETA: 31:34 - loss: 2.0246 - accuracy: 0.5558WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0449s vs `on_train_batch_end` time: 2.0407s). Check your callbacks.\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9879\n",
      "Epoch 1, Test Loss: 0.03254542872309685\n",
      "938/938 [==============================] - 1972s 2s/step - loss: 0.0325 - accuracy: 0.9889 - val_loss: 0.0325 - val_accuracy: 0.9889\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 定义ResNet模块\n",
    "class ResNetBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=5, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "\n",
    "        self.conv2 = layers.Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "\n",
    "        # 1x1 convolutional layer for the residual connection\n",
    "        if stride > 1:\n",
    "            self.residual_conv = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')\n",
    "        else:\n",
    "            self.residual_conv = None\n",
    "\n",
    "        self.relu2 = layers.Activation('relu')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "\n",
    "        # Apply residual connection if needed\n",
    "        if self.residual_conv is not None:\n",
    "            inputs = self.residual_conv(inputs)\n",
    "\n",
    "        x = layers.add([inputs, x])\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "# 构建卷积神经网络\n",
    "def build_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # 第一个卷积层\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # ResNet模块\n",
    "    model.add(ResNetBlock(32))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # 全连接层\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # 输出层\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# 加载MNIST数据集\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# 构建模型\n",
    "model = build_cnn_model((28, 28, 1))\n",
    "\n",
    "# 定义一个回调类来获取每个 mini-batch 后的训练损失和测试损失\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        test_loss = self.model.evaluate(test_images, test_labels, verbose=0)[0]\n",
    "        self.test_losses.append(test_loss)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        test_loss = self.model.evaluate(test_images, test_labels, verbose=0)[0]\n",
    "        self.test_losses.append(test_loss)\n",
    "        print(f'\\nEpoch {epoch + 1}, Test Loss: {test_loss}')\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 实例化 LossHistory 回调类\n",
    "loss_history = LossHistory()\n",
    "\n",
    "# 训练模型，并将 LossHistory 作为回调传递\n",
    "history = model.fit(train_images, train_labels, epochs=1, batch_size=64, validation_data=(test_images, test_labels), callbacks=[loss_history])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6132b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x00000242CC8FF930>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf1045f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 26, 26, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " res_net_block (ResNetBlock  (None, 26, 26, 32)        51520     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1179776   \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1252298 (4.78 MB)\n",
      "Trainable params: 1251722 (4.77 MB)\n",
      "Non-trainable params: 576 (2.25 KB)\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0325 - accuracy: 0.9889\n",
      "\n",
      "Final Test Loss: 0.03254542872309685, Test Accuracy: 0.9889000058174133\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0274 - accuracy: 0.9913\n",
      "Final Training Loss: 0.027373842895030975, Training Accuracy: 0.9913333058357239\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Class Accuracy:\n",
      "    Class 0   Class 1   Class 2  Class 3  Class 4   Class 5   Class 6  \\\n",
      "0  0.995918  0.989427  0.994186  0.99703  0.99389  0.986547  0.990605   \n",
      "\n",
      "    Class 7   Class 8   Class 9  \n",
      "0  0.978599  0.994867  0.968285  \n"
     ]
    }
   ],
   "source": [
    "# 打印模型概要\n",
    "model.summary()\n",
    "\n",
    "# 输出最终测试损失和准确率\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'\\nFinal Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n",
    "\n",
    "# 输出最终训练损失和准确率\n",
    "train_loss, train_acc = model.evaluate(train_images, train_labels)\n",
    "print(f'Final Training Loss: {train_loss}, Training Accuracy: {train_acc}')\n",
    "\n",
    "# 计算测试集中每一类的准确率\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = tf.argmax(predictions, axis=1)\n",
    "true_labels = tf.argmax(test_labels, axis=1)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(true_labels, predicted_labels, num_classes=10)\n",
    "class_accuracy = tf.linalg.diag_part(confusion_matrix) / tf.reduce_sum(confusion_matrix, axis=1)\n",
    "\n",
    "class_accuracy_dict = {f'Class {i}': [acc.numpy()] for i, acc in enumerate(class_accuracy)}\n",
    "\n",
    "# 创建 Pandas DataFrame\n",
    "df = pd.DataFrame(class_accuracy_dict)\n",
    "\n",
    "# 显示表格\n",
    "print(\"Class Accuracy:\")\n",
    "print(df)\n",
    "\n",
    "# 将表格写入 CSV 文件\n",
    "df.to_csv('class_accuracy_5.csv', index=False)\n",
    "\n",
    "# 绘制条形图\n",
    "plt.figure(figsize=(10, 6))  # 设置图形大小\n",
    "\n",
    "bars = plt.bar(df.columns, df.iloc[0])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Class-wise Accuracy with 5x5 Convolutional Kernel')  # 添加标题\n",
    "plt.xticks(rotation=45)  # 使x轴标签斜着显示\n",
    "\n",
    "# 在条形图上显示具体的数值\n",
    "for bar, label in zip(bars, df.iloc[0]):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2 - 0.15, bar.get_height() + 0.01, f'{label:.2%}', ha='center', va='bottom')\n",
    "\n",
    "# 保存条形图\n",
    "plt.savefig('class_accuracy_bar_chart_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959374c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练过程中的损失曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 绘制训练损失曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history.train_losses, label='Training Loss')\n",
    "plt.xlabel('Mini-Batch / Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在每50个点（i < 600）或每200个点（i >= 600）上显示具体的数值\n",
    "for i, value in enumerate(loss_history.train_losses):\n",
    "    if i < 600 and i % 50 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "    elif i >= 600 and (i - 600) % 200 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 绘制测试损失曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history.test_losses, label='Test Loss', color='orange')\n",
    "plt.xlabel('Mini-Batch / Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在每50个点（i < 600）或每200个点（i >= 600）上显示具体的数值\n",
    "for i, value in enumerate(loss_history.test_losses):\n",
    "    if i < 600 and i % 50 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "    elif i >= 600 and (i - 600) % 200 == 0:\n",
    "        plt.text(i, value, f'{value:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 在图形上方添加总标题\n",
    "plt.suptitle('Loss Curves with 5x5 Convolutional Kernel', fontsize=16, y=1)\n",
    "\n",
    "# 保存损失曲线图\n",
    "plt.savefig('loss_curves_5.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
